{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsEFZAr2lB+D4TouCJoPDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kilos11/PYTHON-_AUTOMATION-/blob/main/12_WEB_SCRAPING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: mapIt.py with the webbrowser Module**#\n",
        "##*The webbrowser module’s open() function can launch a new browser to a specified URL."
      ],
      "metadata": {
        "id": "hERhuAGCVTUQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGGrmNVIUdF4",
        "outputId": "63b26480-2cd7-4792-cc4e-dd94615228a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import webbrowser\n",
        "\n",
        "webbrowser.open('https://inventwithpython.com/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*A web browser tab will open to the URL https://inventwithpython.com/. This is about the only thing the webbrowser module can do. Even so, the open() function does make some interesting things possible. For example, it’s tedious to copy a street address to the clipboard and bring up a map of it on Google Maps. You could take a few steps out of this task by writing a simple script to automatically launch the map in your browser using the contents of your clipboard. This way, you only have to copy the address to a clipboard and run the script, and the map will be loaded for you.\n",
        "\n",
        "##*This is what your program does:\n",
        "\n",
        "##*Gets a street address from the command line arguments or clipboard\n",
        "##*Opens the web browser to the Google Maps page for the address\n",
        "##*This means your code will need to do the following:\n",
        "\n",
        "##*Read the command line arguments from sys.argv.\n",
        "##*Read the clipboard contents.\n",
        "##*Call the webbrowser.open() function to open the web browser."
      ],
      "metadata": {
        "id": "PzWMkat3WC92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Figure Out the URL**#\n",
        "##*Based on the instructions in Appendix B, set up mapIt.py so that when you run it from the command line, like so . . .\n",
        "\n",
        "C:\\> mapit 870 Valencia St, San Francisco, CA 94110\n",
        "\n",
        "##* . . the script will use the command line arguments instead of the clipboard. If there are no command line arguments, then the program will know to use the contents of the clipboard.\n",
        "\n",
        "##*First you need to figure out what URL to use for a given street address. When you load https://maps.google.com/ in the browser and search for an address, the URL in the address bar looks something like this: https://www.google.com/maps/place/870+Valencia+St/@37.7590311,-122.4215096,17z/data=!3m1!4b1!4m2!3m1!1s0x808f7e3dadc07a37:0xc86b0b2bb93b73d8.\n",
        "\n",
        "##*The address is in the URL, but there’s a lot of additional text there as well. Websites often add extra data to URLs to help track visitors or customize sites. But if you try just going to https://www.google.com/maps/place/870+Valencia+St+San+Francisco+CA/, you’ll find that it still brings up the correct page. So your program can be set to open a web browser to 'https://www.google.com/maps/place/your_address_string' (where your_address_string is the address you want to map)."
      ],
      "metadata": {
        "id": "k_9CsIeGBl5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Handle the Command Line Arguments**#"
      ],
      "metadata": {
        "id": "YibVT7byChFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser, sys\n",
        "\n",
        "if len(sys.argv) > 1:\n",
        "    # Get address from command line.\n",
        "    address = ' '.join(sys.argv[1:])"
      ],
      "metadata": {
        "id": "KawYqx6nCwC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The sys.argv variable stores a list of the program’s filename and command line arguments. If this list has more than just the filename in it, then len(sys.argv) evaluates to an integer greater than 1, meaning that command line arguments have indeed been provided.\n",
        "\n",
        "##*Command line arguments are usually separated by spaces, but in this case, you want to interpret all of the arguments as a single string. Since sys.argv is a list of strings, you can pass it to the join() method, which returns a single string value. You don’t want the program name in this string, so instead of sys.argv, you should pass sys.argv[1:] to chop off the first element of the array. The final string that this expression evaluates to is stored in the address variable.\n",
        "\n",
        "##*If you run the program by entering this into the command line . . .\n",
        "\n",
        "mapit 870 Valencia St, San Francisco, CA 94110\n",
        "\n",
        ". . . the sys.argv variable will contain this list value:\n",
        "\n",
        "['mapIt.py', '870', 'Valencia', 'St, ', 'San', 'Francisco, ', 'CA', '94110']\n",
        "\n",
        "##*The address variable will contain the string '870 Valencia St, San Francisco, CA 94110'."
      ],
      "metadata": {
        "id": "au00rHgRFaQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Step 3: Handle the Clipboard Content and Launch the Browser**#"
      ],
      "metadata": {
        "id": "C-IR3cIqGNqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser, sys, pyperclip\n",
        "\n",
        "if len(sys.argv) > 1:\n",
        "     # Get address from command line.\n",
        "     address = ' '.join(sys.argv[1:])\n",
        "else:\n",
        "    # Get address from clipboard.\n",
        "    address = pyperclip.paste()\n",
        "webbrowser.open('https://www.google.com/maps/place/' + address)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veS3MbmJGd-b",
        "outputId": "7ac3050f-c327-40d1-bbe4-f46bea1647c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Downloading Files from the Web with the requests Module**#\n",
        "##*The requests module lets you easily download files from the web without having to worry about complicated issues such as network errors, connection problems, and data compression. The requests module doesn’t come with Python, so you’ll have to install it first. From the command line, run pip install --user requests."
      ],
      "metadata": {
        "id": "GKVXitc8JBiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  requests."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1G6xeEyJPCQ",
        "outputId": "ede42f98-e596-4e98-ed1a-af005a40f82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'requests.'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "qAtoo2UyJhFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Downloading a Web Page with the requests.get() Function**#\n",
        "##*The requests.get() function takes a string of a URL to download. By calling type() on requests.get()’s return value, you can see that it returns a Response object, which contains the response that the web server gave for your request."
      ],
      "metadata": {
        "id": "4Xv6tIucJ2dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
        "type(res)\n",
        "res.status_code == requests.codes.ok\n",
        "print(len(res.text))\n",
        "print(res.text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEb-AJZDKFf7",
        "outputId": "e26205c3-b53a-457d-dce9-920c4a2b5677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "178978\n",
            "The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it under the terms of the Project Gutenberg License included\r\n",
            "with this eBook or online at www.gutenberg.org/license\r\n",
            "\r\n",
            "\r\n",
            "Title: Romeo and Juliet\r\n",
            "\r\n",
            "Author: William Shakespeare\r\n",
            "\r\n",
            "Posting Date: May 25, 2012 [EBook #1112]\r\n",
            "Release Date: November, 1997  [Etext #1112]\r\n",
            "\r\n",
            "Language: English\r\n",
            "\r\n",
            "\r\n",
            "*** START OF THIS PROJECT GUTENBERG EBOOK ROMEO AND JULIET ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "*Project Gutenberg is proud to cooperate with The World Library*\r\n",
            "in the presentation of The Complete Works of William Shakespeare\r\n",
            "for your reading for education and entertainment.  HOWEVER, THIS\r\n",
            "IS NEITHER SHAREWARE NOR PUBLIC DOMAIN. . .AND UNDER THE LIBRARY\r\n",
            "OF THE FUTURE CONDITIONS OF THIS PRESENTATION. . .NO CHARGES MAY\r\n",
            "BE MADE FOR *ANY* ACCESS TO THIS MATERIAL.  YOU ARE ENCOURAGED!!\r\n",
            "TO G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The URL goes to a text web page for the entire play of Romeo and Juliet, provided on this book’s site ➊. You can tell that the request for this web page succeeded by checking the status_code attribute of the Response object. If it is equal to the value of requests.codes.ok, then everything went fine ➋. (Incidentally, the status code for “OK” in the HTTP protocol is 200. You may already be familiar with the 404 status code for “Not Found.”) You can find a complete list of HTTP status codes and their meanings at https://en.wikipedia.org/wiki/List_of_HTTP_status_codes.\n",
        "\n",
        "##*If the request succeeded, the downloaded web page is stored as a string in the Response object’s text variable. This variable holds a large string of the entire play; the call to len(res.text) shows you that it is more than 178,000 characters long. Finally, calling print(res.text[:250]) displays only the first 250 characters.\n",
        "\n",
        "##*If the request failed and displayed an error message, like “Failed to establish a new connection” or “Max retries exceeded,” then check your internet connection. Connecting to servers can be quite complicated, and I can’t give a full list of possible problems here. You can find common causes of your error by doing a web search of the error message in quotes."
      ],
      "metadata": {
        "id": "aedzqV9yZCEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get('https://inventwithpython.com/page_that_does_not_exist')\n",
        "res.raise_for_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "xKtEGwRCalcw",
        "outputId": "87f2dbf8-0e83-4ea8-966b-8029cf08914d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "404 Client Error: Not Found for url: https://inventwithpython.com/page_that_does_not_exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e43daf6a44f2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://inventwithpython.com/page_that_does_not_exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://inventwithpython.com/page_that_does_not_exist"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The raise_for_status() method is a good way to ensure that a program halts if a bad download occurs. This is a good thing: You want your program to stop as soon as some unexpected error happens. If a failed download isn’t a deal breaker for your program, you can wrap the raise_for_status() line with try and except statements to handle this error case without crashing."
      ],
      "metadata": {
        "id": "afk5NNQia9KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get('https://inventwithpython.com/page_that_does_not_exist')\n",
        "try:\n",
        "    res.raise_for_status()\n",
        "except Exception as exc:\n",
        "    print('There was a problem: %s' % (exc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2vg_OGibGMx",
        "outputId": "fcf474e3-db76-4cd9-e6b8-1d0e884c935d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There was a problem: 404 Client Error: Not Found for url: https://inventwithpython.com/page_that_does_not_exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*This raise_for_status() method call causes the program to output the following:\n",
        "\n",
        "There was a problem: 404 Client Error: Not Found for url: https://\n",
        "inventwithpython.com/page_that_does_not_exist.html\n",
        "\n",
        "##*Always call raise_for_status() after calling requests.get(). You want to be sure that the download has actually worked before your program continues."
      ],
      "metadata": {
        "id": "52Dhe0z4bvSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Saving Downloaded Files to the Hard Drive**#\n",
        "##*From here, you can save the web page to a file on your hard drive with the standard open() function and write() method. There are some slight differences, though. First, you must open the file in write binary mode by passing the string 'wb' as the second argument to open(). Even if the page is in plaintext (such as the Romeo and Juliet text you downloaded earlier), you need to write binary data instead of text data in order to maintain the Unicode encoding of the text.\n",
        "\n",
        "##*To write the web page to a file, you can use a for loop with the Response object’s iter_content() method."
      ],
      "metadata": {
        "id": "QyFqeo0ccJQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
        "res.raise_for_status()\n",
        "playFile = open('RomeoAndJuliet.txt', 'wb')\n",
        "for chunk in res.iter_content(100000):\n",
        "    print(playFile.write(chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBs7x_MEcy-c",
        "outputId": "173c5d47-1b3c-4a06-e07e-0ad1d2358546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000\n",
            "78978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The iter_content() method returns “chunks” of the content on each iteration through the loop. Each chunk is of the bytes data type, and you get to specify how many bytes each chunk will contain. One hundred thousand bytes is generally a good size, so pass 100000 as the argument to iter_content().\n",
        "\n",
        "##*The file RomeoAndJuliet.txt will now exist in the current working directory. Note that while the filename on the website was rj.txt, the file on your hard drive has a different filename. The requests module simply handles downloading the contents of web pages. Once the page is downloaded, it is simply data in your program. Even if you were to lose your internet connection after downloading the web page, all the page data would still be on your computer."
      ],
      "metadata": {
        "id": "B3CXo5wsdsaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The write() method returns the number of bytes written to the file. In the previous example, there were 100,000 bytes in the first chunk, and the remaining part of the file needed only 78,981 bytes.\n",
        "\n",
        "##*To review, here’s the complete process for downloading and saving a file:\n",
        "\n",
        "##*Call requests.get() to download the file.\n",
        "##*Call open() with 'wb' to create a new file in write binary mode.\n",
        "##*Loop over the Response object’s iter_content() method.\n",
        "##*Call write() on each iteration to write the content to the file.\n",
        "##*Call close() to close the file.\n",
        "##*That’s all there is to the requests module! The for loop and iter_content() stuff may seem complicated compared to the open()/write()/close() workflow you’ve been using to write text files, but it’s to ensure that the requests module doesn’t eat up too much memory even if you download massive files. You can learn about the requests module’s other features from https://requests.readthedocs.org/."
      ],
      "metadata": {
        "id": "ZRcL7AgXfg4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**HTML**#\n",
        "##*Before you pick apart web pages, you’ll learn some HTML basics. You’ll also see how to access your web browser’s powerful developer tools, which will make scraping information from the web much easier.\n",
        "\n",
        "##*A Quick Refresher\n",
        "##*In case it’s been a while since you’ve looked at any HTML, here’s a quick overview of the basics. An HTML file is a plaintext file with the .html file extension. The text in these files is surrounded by tags, which are words enclosed in angle brackets. The tags tell the browser how to format the web page. A starting tag and closing tag can enclose some text to form an element. The text (or inner HTML) is the content between the starting and closing tags. For example, the following HTML will display Hello, world! in the browser, with Hello in bold:\n",
        "\n",
        "##**<strong>Hello</strong>, world!"
      ],
      "metadata": {
        "id": "qb2si9KDiLgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**The opening <strong> tag says that the enclosed text will appear in bold. The closing </strong> tags tells the browser where the end of the bold text is.\n",
        "\n",
        "##*There are many different tags in HTML. Some of these tags have extra properties in the form of attributes within the angle brackets. For example, the <a> tag encloses text that should be a link. The URL that the text links to is determined by the href attribute. Here’s an example:\n",
        "\n",
        "\"Al's free <a href=\"https://inventwithpython.com\">Python books</a>.\"\n",
        "##*Some elements have an id attribute that is used to uniquely identify the element in the page. You will often instruct your programs to seek out an element by its id attribute, so figuring out an element’s id attribute using the browser’s developer tools is a common task in writing web scraping programs."
      ],
      "metadata": {
        "id": "lJqBj3SCjsb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Viewing the Source HTML of a Web Page\n",
        "##*You’ll need to look at the HTML source of the web pages that your programs will work with. To do this, right-click (or CTRL-click on macOS) any web page in your web browser, and select View Source or View page source to see the HTML text of the page (see Figure 12-3). This is the text your browser actually receives. The browser knows how to display, or render, the web page from this HTML.\n",
        "##*Using the Developer Tools to Find HTML Elements\n",
        "##*Once your program has downloaded a web page using the requests module, you will have the page’s HTML content as a single string value. Now you need to figure out which part of the HTML corresponds to the information on the web page you’re interested in.\n",
        "\n",
        "##*This is where the browser’s developer tools can help. Say you want to write a program to pull weather forecast data from https://weather.gov/. Before writing any code, do a little research. If you visit the site and search for the 94105 ZIP code, the site will take you to a page showing the forecast for that area.\n",
        "\n",
        "##*What if you’re interested in scraping the weather information for that ZIP code? Right-click where it is on the page (or CONTROL-click on macOS) and select Inspect Element from the context menu that appears. This will bring up the Developer Tools window, which shows you the HTML that produces this particular part of the web page. Figure 12-5 shows the developer tools open to the HTML of the nearest forecast. Note that if the https://weather.gov/ site changes the design of its web pages, you’ll need to repeat this process to inspect the new elements.\n",
        "##*From the developer tools, you can see that the HTML responsible for the forecast part of the web page is <div class=\"col-sm-10 forecast-text\">Sunny, with a high near 64. West wind 11 to 16 mph, with gusts as high as 21 mph.</div>. This is exactly what you were looking for! It seems that the forecast information is contained inside a <div> element with the forecast-text CSS class. Right-click on this element in the browser’s developer console, and from the context menu that appears, select Copy ▸ CSS Selector. This will copy a string such as 'div.row-odd:nth-child(1) > div:nth-child(2)' to the clipboard. You can use this string for Beautiful Soup’s select() or Selenium’s find_element_by_css_selector() methods, as explained later in this chapter. Now that you know what you’re looking for, the Beautiful Soup module will help you find it in the string.\n",
        "\n",
        "<!-- This is the example.html example file. -->\n",
        "\n",
        "<html><head><title>The Website Title</title></head>\n",
        "<body>\n",
        "<p>Download my <strong>Python</strong> book from <a href=\"https://\n",
        "inventwithpython.com\">my website</a>.</p>\n",
        "<p class=\"slogan\">Learn Python the easy way!</p>\n",
        "<p>By <span id=\"author\">Al Sweigart</span></p>\n",
        "</body></html>"
      ],
      "metadata": {
        "id": "Ln9paYXrp4_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Creating a BeautifulSoup Object from HTML**#\n",
        "##*The bs4.BeautifulSoup() function needs to be called with a string containing the HTML it will parse. The bs4.BeautifulSoup() function returns a BeautifulSoup object"
      ],
      "metadata": {
        "id": "9t1qGSJcs1T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFaO3WcItO26",
        "outputId": "2e722707-4481-4a53-822f-ab27ac891cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, bs4\n",
        "\n",
        "res = requests.get('https://nostarch.com')\n",
        "res.raise_for_status()\n",
        "noStarchSoup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
        "type(noStarchSoup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "uuPxn88XtfMu",
        "outputId": "4b48cab6-c128-4261-e860-93f71c5a7275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>bs4.BeautifulSoup</b><br/>def __call__(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/bs4/__init__.py</a>A data structure representing a parsed HTML or XML document.\n",
              "\n",
              "Most of the methods you&#x27;ll call on a BeautifulSoup object are inherited from\n",
              "PageElement or Tag.\n",
              "\n",
              "Internally, this class defines the basic interface called by the\n",
              "tree builders when converting an HTML/XML document into a data\n",
              "structure. The interface abstracts away the differences between\n",
              "parsers. To write a new tree builder, you&#x27;ll need to understand\n",
              "these methods as a whole.\n",
              "\n",
              "These methods will be called by the BeautifulSoup constructor:\n",
              "  * reset()\n",
              "  * feed(markup)\n",
              "\n",
              "The tree builder may call these methods from its feed() implementation:\n",
              "  * handle_starttag(name, attrs) # See note about return value\n",
              "  * handle_endtag(name)\n",
              "  * handle_data(data) # Appends to the current data node\n",
              "  * endData(containerClass) # Ends the current data node\n",
              "\n",
              "No matter how complicated the underlying parser is, you should be\n",
              "able to build a tree using &#x27;start tag&#x27; events, &#x27;end tag&#x27; events,\n",
              "&#x27;data&#x27; events, and &quot;done with data&quot; events.\n",
              "\n",
              "If you encounter an empty-element tag (aka a self-closing tag,\n",
              "like HTML&#x27;s &lt;br&gt; tag), call handle_starttag and then\n",
              "handle_endtag.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 76);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*This code uses requests.get() to download the main page from the No Starch Press website and then passes the text attribute of the response to bs4.BeautifulSoup(). The BeautifulSoup object that it returns is stored in a variable named noStarchSoup.\n",
        "\n",
        "##*You can also load an HTML file from your hard drive by passing a File object to bs4.BeautifulSoup() along with a second argument that tells Beautiful Soup which parser to use to analyze the HTML."
      ],
      "metadata": {
        "id": "c2j4647BulWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exampleFile = open('example.html')\n",
        "exampleSoup = bs4.BeautifulSoup(exampleFile, 'html.parser')\n",
        "type(exampleSoup)"
      ],
      "metadata": {
        "id": "B4WYzeMsu9V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Finding an Element with the select() Method**#\n",
        "##*You can retrieve a web page element from a BeautifulSoup object by calling the select()method and passing a string of a CSS selector for the element you are looking for. Selectors are like regular expressions: they specify a pattern to look for—in this case, in HTML pages instead of general text strings.\n",
        "\n",
        "Examples of CSS Selectors\n",
        "\n",
        "Selector passed to the select() method\n",
        "\n",
        "Will match . . .\n",
        "\n",
        "##*soup.select('div')\n",
        "\n",
        "All elements named\n",
        "\n",
        "##*soup.select('#author')\n",
        "\n",
        "The element with an id attribute of author\n",
        "\n",
        "##*soup.select('.notice')\n",
        "\n",
        "All elements that use a CSS class attribute named notice\n",
        "\n",
        "##*soup.select('div span')\n",
        "\n",
        "All elements named <span> that are within an element named <div>\n",
        "\n",
        "##*soup.select('div > span')\n",
        "\n",
        "All elements named <span> that are directly within an element named <div>, with no other element in between\n",
        "\n",
        "##*soup.select('input[name]')\n",
        "\n",
        "All elements named <input> that have a name attribute with any value\n",
        "\n",
        "##*soup.select('input[type=\"button\"]')\n",
        "\n",
        "All elements named <input> that have an attribute named type with value button"
      ],
      "metadata": {
        "id": "7y5OZf9exOS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*The various selector patterns can be combined to make sophisticated matches. For example, soup.select('p #author') will match any element that has an id attribute of author, as long as it is also inside a \"p\" element. Instead of writing the selector yourself, you can also right-click on the element in your browser and select Inspect Element. When the browser’s developer console opens, right-click on the element’s HTML and select Copy ▸ CSS Selector to copy the selector string to the clipboard and paste it into your source code.\n",
        "\n",
        "##*The select() method will return a list of Tag objects, which is how Beautiful Soup represents an HTML element. The list will contain one Tag object for every match in the BeautifulSoup object’s HTML. Tag values can be passed to the str() function to show the HTML tags they represent. Tag values also have an attrs attribute that shows all the HTML attributes of the tag as a dictionary."
      ],
      "metadata": {
        "id": "gMTU80efz9K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "\n",
        "exampleFile = open('example.html')\n",
        "exampleSoup = bs4.BeautifulSoup(exampleFile.read(), 'html.parser')\n",
        "elems = exampleSoup.select('#author')\n",
        "type(elems)"
      ],
      "metadata": {
        "id": "3D57IJrv09HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Getting Data from an Element’s Attributes*#\n",
        "##*The get() method for Tag objects makes it simple to access attribute values from an element. The method is passed a string of an attribute name and returns that attribute’s value."
      ],
      "metadata": {
        "id": "zihxxo7G1pi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "\n",
        "soup = bs4.BeautifulSoup(open('example.html'), 'html.parser')\n",
        "spanElem = soup.select('span')[0]\n",
        "str(spanElem)\n",
        "\n",
        "spanElem.get('id')\n",
        "spanElem.get('some_nonexistent_addr') == None\n",
        "spanElem.attrs"
      ],
      "metadata": {
        "id": "rX01dkgz11Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: Opening All Search Results**#\n",
        "##*Whenever I search a topic on Google, I don’t look at just one search result at a time. By middle-clicking a search result link (or clicking while holding CTRL), I open the first several links in a bunch of new tabs to read later. I search Google often enough that this workflow—opening my browser, searching for a topic, and middle-clicking several links one by one—is tedious. It would be nice if I could simply type a search term on the command line and have my computer automatically open a browser with all the top search results in new tabs. Let’s write a script to do this with the search results page for the Python Package Index at https://pypi.org/. A program like this can be adapted to many other websites, although the Google and DuckDuckGo often employ measures that make scraping their search results pages difficult.\n",
        "\n",
        "##*This is what your program does:\n",
        "\n",
        "##*Gets search keywords from the command line arguments\n",
        "##*Retrieves the search results page\n",
        "##*Opens a browser tab for each result\n",
        "##*This means your code will need to do the following:\n",
        "\n",
        "##*Read the command line arguments from sys.argv.\n",
        "##*Fetch the search result page with the requests module.\n",
        "##*Find the links to each search result.\n",
        "##*Call the webbrowser.open() function to open the web browser."
      ],
      "metadata": {
        "id": "8sBrJqe-yRPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Get the Command Line Arguments and Request the Search Page**#\n",
        "##*Before coding anything, you first need to know the URL of the search result page. By looking at the browser’s address bar after doing a search, you can see that the result page has a URL like https://pypi.org/search/?q=<SEARCH_TERM_HERE>. The requests module can download this page and then you can use Beautiful Soup to find the search result links in the HTML. Finally, you’ll use the webbrowser module to open those links in browser tabs."
      ],
      "metadata": {
        "id": "BmOAk972zDfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, sys, webbrowser, bs4\n",
        "\n",
        "print('Searching...')    # display text while downloading the search result page\n",
        "res = requests.get('https://google.com/search?q=' 'https://pypi.org/search/?q='\n",
        "+ ' '.join(sys.argv[1:]))\n",
        "res.raise_for_status()\n",
        "# TODO: Retrieve top search result links.\n",
        "\n",
        "# TODO: Open a browser tab for each result."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIF9gerBzSKf",
        "outputId": "66a02779-9877-4b42-b73b-cfd7744dd6a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Find All the Results**#\n",
        "##*Now you need to use Beautiful Soup to extract the top search result links from your downloaded HTML. But how do you figure out the right selector for the job? For example, you can’t just search for all <a> tags, because there are lots of links you don’t care about in the HTML. Instead, you must inspect the search result page with the browser’s developer tools to try to find a selector that will pick out only the links you want.\n",
        "\n",
        "##*After doing a search for Beautiful Soup, you can open the browser’s developer tools and inspect some of the link elements on the page. They can look complicated, something like pages of this: <a class=\"package-snippet\" href=\"/project/pyautogui/\">.\n",
        "\n",
        "##*It doesn’t matter that the element looks incredibly complicated. You just need to find the pattern that all the search result links have."
      ],
      "metadata": {
        "id": "hxrdIrCyz6hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, sys, webbrowser, bs4\n",
        "\n",
        "# Retrieve top search result links.\n",
        "soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
        "# Open a browser tab for each result.\n",
        "linkElems = soup.select('.package-snippet')"
      ],
      "metadata": {
        "id": "j_OItlx50VA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Open Web Browsers for Each Result**#\n",
        "##*Finally, we’ll tell the program to open web browser tabs for our results."
      ],
      "metadata": {
        "id": "hdv-tiz40rhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, sys, webbrowser, bs4\n",
        "\n",
        "# Open a browser tab for each result.\n",
        "linkElems = soup.select('.package-snippet')\n",
        "numOpen = min(5, len(linkElems))\n",
        "for i in range(numOpen):\n",
        "    urlToOpen = 'https://pypi.org' + linkElems[i].get('href')\n",
        "    print('Opening', urlToOpen)\n",
        "    webbrowser.open(urlToOpen)"
      ],
      "metadata": {
        "id": "J5SY_c-J01i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project: Downloading All XKCD Comics**#\n",
        "##*Blogs and other regularly updating websites usually have a front page with the most recent post as well as a Previous button on the page that takes you to the previous post. Then that post will also have a Previous button, and so on, creating a trail from the most recent page to the first post on the site. If you wanted a copy of the site’s content to read when you’re not online, you could manually navigate over every page and save each one. But this is pretty boring work, so let’s write a program to do it instead.\n",
        "\n",
        "##*XKCD is a popular geek webcomic with a website that fits this structure (see Figure 12-6). The front page at https://xkcd.com/ has a Prev button that guides the user back through prior comics. Downloading each comic by hand would take forever, but you can write a script to do this in a couple of minutes.\n",
        "##**Here’s what your program does:\n",
        "\n",
        "##*Loads the XKCD home page\n",
        "##*Saves the comic image on that page\n",
        "##*Follows the Previous Comic link\n",
        "##*Repeats until it reaches the first comic\n",
        "##**This means your code will need to do the following:\n",
        "\n",
        "##*Download pages with the requests module.\n",
        "##*Find the URL of the comic image for a page using Beautiful Soup.\n",
        "##*Download and save the comic image to the hard drive with iter_content().\n",
        "##*Find the URL of the Previous Comic link, and repeat."
      ],
      "metadata": {
        "id": "mY7zBVXZ1yOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Design the Program**#\n",
        "##**If you open the browser’s developer tools and inspect the elements on the page, you’ll find the following:\n",
        "\n",
        "##*The URL of the comic’s image file is given by the href attribute of an <img> element.\n",
        "##*The <img> element is inside a <div id=\"comic\"> element.\n",
        "##*The Prev button has a rel HTML attribute with the value prev.\n",
        "##*The first comic’s Prev button links to the https://xkcd.com/# URL, indicating that there are no more previous pages."
      ],
      "metadata": {
        "id": "B-BbDIdu4NUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! python3\n",
        "# downloadXkcd.py - Downloads every single XKCD comic.\n",
        "\n",
        "import requests, os, bs4\n",
        "\n",
        "url = 'https://xkcd.com'               # starting url\n",
        "os.makedirs('xkcd', exist_ok=True)    # store comics in ./xkcd\n",
        "while not url.endswith('#'):\n",
        "    # TODO: Download the page.\n",
        "\n",
        "        # TODO: Find the URL of the comic image.\n",
        "\n",
        "            # TODO: Download the image.\n",
        "\n",
        "                # TODO: Save the image to ./xkcd.\n",
        "\n",
        "                    # TODO: Get the Prev button's url.\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "4KydQmb54wae",
        "outputId": "aed4ea61-be46-4c3f-c870-cfc3fede59b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'while' statement on line 8 (<ipython-input-6-a7bca630ecb4>, line 18)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a7bca630ecb4>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print('Done.')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'while' statement on line 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*You’ll have a url variable that starts with the value 'https://xkcd.com' and repeatedly update it (in a for loop) with the URL of the current page’s Prev link. At every step in the loop, you’ll download the comic at url. You’ll know to end the loop when url ends with '#'.\n",
        "\n",
        "##*You will download the image files to a folder in the current working directory named xkcd. The call os.makedirs() ensures that this folder exists, and the exist_ok=True keyword argument prevents the function from throwing an exception if this folder already exists. The remaining code is just comments that outline the rest of your program."
      ],
      "metadata": {
        "id": "Lqgyq7Mc5ljH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 2: Download the Web Page**#\n",
        "##*Let’s implement the code for downloading the page."
      ],
      "metadata": {
        "id": "lPlFdD1i5xUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! python3\n",
        "# downloadXkcd.py - Downloads every single XKCD comic.\n",
        "\n",
        "import requests, os, bs4\n",
        "\n",
        "url = 'https://xkcd.com'               # starting url\n",
        "os.makedirs('xkcd', exist_ok=True)    # store comics in ./xkcd\n",
        "while not url.endswith('#'):\n",
        "    # Download the page.\n",
        "    print('Downloading page %s...' % url)\n",
        "    res = requests.get(url)\n",
        "    res.raise_for_status()\n",
        "\n",
        "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
        "    print('Done.')"
      ],
      "metadata": {
        "id": "8E0LsxUU6xkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*First, print url so that the user knows which URL the program is about to download; then use the requests module’s request.get() function to download it. As always, you immediately call the Response object’s raise_for_status() method to throw an exception and end the program if something went wrong with the download. Otherwise, you create a BeautifulSoup object from the text of the downloaded page."
      ],
      "metadata": {
        "id": "ZigM8nt77m-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Find and Download the Comic Image**#"
      ],
      "metadata": {
        "id": "W9xr-lXT74HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! python3\n",
        "# downloadXkcd.py - Downloads every single XKCD comic.\n",
        "\n",
        "import requests, os, bs4\n",
        "\n",
        " # Find the URL of the comic image.\n",
        "comicElem = soup.select('#comic img')\n",
        "if comicElem == []:\n",
        "    print('Could not find comic image.')\n",
        "else:\n",
        "    comicUrl = 'https:' + comicElem[0].get('src')\n",
        "    # Download the image.\n",
        "    print('Downloading image %s...' % (comicUrl))\n",
        "    res = requests.get(comicUrl)\n",
        "    res.raise_for_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmtofc2J8GyX",
        "outputId": "76ab81ea-9301-400b-825c-4bdfb5f7c83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading image https://imgs.xkcd.com/comics/survey_marker.png...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Save the Image and Find the Previous Comic**#"
      ],
      "metadata": {
        "id": "_bP8IbgdQrAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! python3\n",
        "# downloadXkcd.py - Downloads every single XKCD comic.\n",
        "\n",
        "import requests, os, bs4\n",
        "\n",
        "# Save the image to ./xkcd.\n",
        "imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)),\n",
        "'wb')\n",
        "for chunk in res.iter_content(100000):\n",
        "    imageFile.write(chunk)\n",
        "imageFile.close()\n",
        " # Get the Prev button's url.\n",
        "prevLink = soup.select('a[rel=\"prev\"]')[0]\n",
        "url = 'https://xkcd.com' + prevLink.get('href')"
      ],
      "metadata": {
        "id": "TlrRhqG4QzSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Controlling the Browser with the selenium Module**#\n",
        "##*The selenium module lets Python directly control the browser by programmatically clicking links and filling in login information, almost as though there were a human user interacting with the page. Using selenium, you can interact with web pages in a much more advanced way than with requests and bs4; but because it launches a web browser, it is a bit slower and hard to run in the background if, say, you just need to download some files from the web.\n",
        "\n",
        "##*Still, if you need to interact with a web page in a way that, say, depends on the JavaScript code that updates the page, you’ll need to use selenium instead of requests. That’s because major ecommerce websites such as Amazon almost certainly have software systems to recognize traffic that they suspect is a script harvesting their info or signing up for multiple free accounts. These sites may refuse to serve pages to you after a while, breaking any scripts you’ve made. The selenium module is much more likely to function on these sites long-term than requests.\n",
        "\n",
        "##*A major “tell” to websites that you’re using a script is the user-agent string, which identifies the web browser and is included in all HTTP requests. For example, the user-agent string for the requests module is something like 'python-requests/2.21.0'. You can visit a site such as https://www.whatsmyua.info/ to see your user-agent string. Using selenium, you’re much more likely to “pass for human” because not only is Selenium’s user-agent is the same as a regular browser (for instance, 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0'), but it has the same traffic patterns: a selenium-controlled browser will download images, advertisements, cookies, and privacy-invading trackers just like a regular browser. However, selenium can still be detected by websites, and major ticketing and ecommerce websites often block browsers controlled by selenium to prevent web scraping of their pages."
      ],
      "metadata": {
        "id": "3MZyMetaSsJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Starting a selenium-Controlled Browser**#\n",
        "##*The following examples will show you how to control Firefox’s web browser. If you don’t already have Firefox, you can download it for free from https://getfirefox.com/. You can install selenium by running pip install --user selenium from a command line terminal."
      ],
      "metadata": {
        "id": "3p1bFhP1TNTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt  install  selenium"
      ],
      "metadata": {
        "id": "ka85phYWUoWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e6cf188-1d1b-482f-879b-03127dcb4f26"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package selenium\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E8bP-OMeXiNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "\n",
        "browser = webdriver.Firefox()\n",
        "type(browser)\n",
        "browser.get('https://inventwithpython.com')"
      ],
      "metadata": {
        "id": "ezeC_ZPBU-zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*You’ll notice when webdriver.Firefox() is called, the Firefox web browser starts up. Calling type() on the value webdriver.Firefox() reveals it’s of the WebDriver data type. And calling browser.get('https://inventwithpython.com') directs the browser to https://inventwithpython.com/."
      ],
      "metadata": {
        "id": "M8blMAzgfixz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Finding Elements on the Page**#\n",
        "##*WebDriver objects have quite a few methods for finding elements on a page. They are divided into the find_element_* and find_elements_* methods. The find_element_* methods return a single WebElement object, representing the first element on the page that matches your query. The find_elements_* methods return a list of WebElement_* objects for every matching element on the page.\n",
        "\n",
        "##**Selenium’s WebDriver Methods for Finding Elements\n",
        "\n",
        "##**Method name\n",
        "\n",
        "##**WebElement object/list returned\n",
        "\n",
        "browser.find_element_by_class_name(name)\n",
        "\n",
        "browser.find_elements_by_class_name(name)\n",
        "\n",
        "##**Elements that use the CSS\n",
        "##**class name\n",
        "\n",
        "browser.find_element_by_css_selector(selector)\n",
        "browser.find_elements_by_css_selector(selector)\n",
        "\n",
        "##**Elements that match the CSS\n",
        "##*selector\n",
        "\n",
        "browser.find_element_by_id(id)\n",
        "\n",
        "browser.find_elements_by_id(id)\n",
        "\n",
        "##**Elements with a matching id\n",
        "##**attribute value\n",
        "\n",
        "browser.find_element_by_link_text(text)\n",
        "\n",
        "browser.find_elements_by_link_text(text)\n",
        "\n",
        "##**<a> elements that completely\n",
        "##**match the text provided\n",
        "\n",
        "browser.find_element_by_partial_link_text(text)\n",
        "\n",
        "browser.find_elements_by_partial_link_text(text)\n",
        "\n",
        "##**<a> elements that contain the\n",
        "##**text provided\n",
        "\n",
        "browser.find_element_by_name(name)\n",
        "\n",
        "browser.find_elements_by_name(name)\n",
        "\n",
        "##**Elements with a matching name\n",
        "##**attribute value\n",
        "\n",
        "browser.find_element_by_tag_name(name)\n",
        "browser.find_elements_by_tag_name(name)\n",
        "\n",
        "##**Elements with a matching tag name\n",
        "(case-insensitive; an <a> element is\n",
        "matched by 'a' and 'A')"
      ],
      "metadata": {
        "id": "KbHkQxJzf7pz"
      }
    }
  ]
}